===============
Scripting Guide
===============

Before scripting
~~~~~~~~~~~~~~~~

1. Get Globus ID
----------------
1. Follow only step 1 of `instructions here <https://docs.globus.org/how-to/get-started/>`_ to get a Globus account.
2. Ensure that your ``globus ID`` is linked with your institutional ID in your globus account:
    a. Log into `globus.org <www.globus.org>`_
    b. Click on ``Account`` on the left hand pane
    c. Select the ``Identities`` tab in the window that opens up
    d. You should see (at least these) two identities:

       i. One from your home institution (that is listed as ``primary`` with a crown)
       ii. Globus ID (your_username@globusid.org)

    e. If you do not see the ``Globus ID``, click on ``Link another identity``. Select ``Globus ID`` and link this ID.

2. Register at DataFed
----------------------
1. Once you have a Globus ID, visit the `DataFed web portal <https://datafed.ornl.gov>`_.
2. Click on the ``Log in / Register`` button on the top right of the page.
3. Follow the steps to register yourself with DataFed.
4. Though you can log into the DataFed web portal with your institution's credentials, you will need the username and password you set up during your registration for scripting.

3. Installing DataFed
---------------------
For this section, we will assume that you intend to use the Client CLI on a
remote machine such as an institutional cluster or HPC and that this machine has one or more Globus endpoints that can be used by all uesrs.

1. Load any python 3.5+ module or any conda environment that you intend to use.
2. Install the datafed client package via:
   ``pip install --user datafed``
3. Try typing ``datafed`` to access the DataFed CLI.
   If you encounter errors stating that datafed was an unknown command, you would need to add DataFed to your path.
   a. First, you would need to find where datafed was installed. For example, in the case of NERSC's Cori machine, datafed was installed at ``~/.local/cori/3.7-anaconda-2019.10/bin``.
   b. Next, add DataFed to the path via ``PATH=$PATH:path/to/datafed/here``. Though this works, this addition to the path is only valid for this shell session.
   It is recommended to add the path to your ``bashrc`` or ``rc`` such that datafed is loaded everytime you log in.

4. Setting up DataFed
---------------------
1. Type ``datafed setup`` into the shell. It will prompt you for your username and password.
2. Enter the credentials you set up when registering for an account on DataFed
3. Identify the Globus endpoint(s) attached to this machine from the user guide for the machine you are using.
   For example, the following endpoint can be used when using OLCF's Summit supercomputer: ``olcf#dtn``
4. Now, add this end point as your default endpoint via:
   ``datafed ep default set endpoint_name_here``

This concludes the one-time setup necessary to get started with scripting using DataFed.
You may use the interactive DataFed CLI or the Python package at this point.

Shell scripting
~~~~~~~~~~~~~~~

Creating a record in data repository

.. code:: bash

    > datafed data create \
    --alias "record_from_nersc" \ # Optional argument
    --description "Data and metadata created at NERSC" \ # Optional argument
    --metadata-file ./nersc_md.json \ # Optional argument
    "First record created at NERSC using DataFed CLI" # Title is required though

    ID:            d/31030353
    Alias:         record_from_nersc
    Title:         First record created at NERSC using DataFed CLI
    Data Size:     0
    Data Repo ID:  repo/cades-cnms
    Source:        (none)
    Owner:         somnaths
    Creator:       somnaths
    Created:       11/25/2020,08:04
    Updated:       11/25/2020,08:04
    Description:   Data and metadata created at NERSC

Checking to make sure record was created

.. code:: bash

    > datafed ls

    1. d/31027390   (record_from_alcf)    First record created at ALCF
    2. d/31030353   (record_from_nersc)   First record created at NERSC using DataFed CLI
    3. d/29426537                         from_olcf

Putting raw data into record (via Globus)

.. code:: bash

    > datafed data put \
      --wait \ # optional - wait until Globus transfer completes
      "record_from_nersc" \ # optional - (unique) alias of record
      ./nersc_data.txt # path to data

    Task ID:             task/31030394
    Type:                Data Put
    Status:              Succeeded
    Started:             11/25/2020,08:05
    Updated:             11/25/2020,08:05

Checking to make sure data was uploaded

.. code:: bash

    > datafed data view "record_from_nersc"

    ID:            d/31030353
    Alias:         record_from_nersc
    Title:         First record created at NERSC using DataFed CLI
    Tags:          (none)
    Data Size:     37.0 B
    Data Repo ID:  repo/cades-cnms
    Source:        nersc#dtn/global/u2/s/somnaths/nersc_data.txt
    Extension:     (auto)
    Owner:         somnaths
    Creator:       somnaths
    Created:       11/25/2020,08:04
    Updated:       11/25/2020,08:05
    Description:   Data and metadata created at NERSC

(Preparing to) get data from repository:

.. code:: bash

    > datafed data view d/10314975

    ID:            d/10314975
    Alias:         cln_b_1_beline_0001
    Title:         CLN_B_1_BEline_0001
    Tags:          (none)
    Data Size:     25.7 MB
    Data Repo ID:  repo/cades-cnms
    Source:        57230a10-7ba2-11e7-8c3b-22000b9923ef/lustre/or-hydra/cades-ccsd/syz/pycroscopy_ensemble/be_sho/Nanophase_Done/CLN_B_1_BEline_0001.h5
    Extension:     (auto)
    Owner:         somnaths
    Creator:       somnaths
    Created:       11/01/2019,19:54
    Updated:       11/15/2019,20:31
    Description:   (none)

Showing that the file we want to download / get doesn't already exist:

.. code:: bash

    > ls -hlt
    total 28M
    -rw-rw---- 1 somnaths somnaths   40 Nov 25 07:58 nersc_md.json
    -rw-r--r-- 1 somnaths somnaths 400K Nov  3 13:36 Translation_compiled.html
    -rw-r--r-- 1 somnaths somnaths 1.9M Nov  3 13:30 image_02.mat
    -rw-rw---- 1 somnaths somnaths   37 Nov  3 11:41 nersc_data.txt

Initiating the the ``get`` command:

.. code:: bash

    > datafed data get \
      --wait \ # optional - wait for Globus transfer to complete
      d/10314975 \ # ID of data record
      . # Where to put it in local file system

    > ls -hlt
    total 28M
    -rw-r--r-- 1 somnaths somnaths  26M Nov 25 08:08 10314975.h5
    -rw-rw---- 1 somnaths somnaths   40 Nov 25 07:58 nersc_md.json
    -rw-r--r-- 1 somnaths somnaths 400K Nov  3 13:36 Translation_compiled.html
    -rw-r--r-- 1 somnaths somnaths 1.9M Nov  3 13:30 image_02.mat
    -rw-rw---- 1 somnaths somnaths   37 Nov  3 11:41 nersc_data.txt


Python scripting
~~~~~~~~~~~~~~~~
Import necessary packages

.. code:: python

    >>> import json
    >>> from datafed.CommandLib import API

Create an instance of the DataFed API:

.. code:: python

    >>> df_api = API()

By default, one would need to get metadata from the simulation / measurement files.
Here, we use fake metadata in place of the real metadata:

.. code:: python

    >>> parameters = {
                      'a': 4,
                      'b': [1, 2, -4, 7.123],
                      'c': 'Something important',
                      'd': {'x': 14, 'y': -19} # Can use nested dictionaries
                      }

Creating the record:
Until the next version of DataFed, which can accept a python dictionary itself instead
of a JSON file or a JSON string for the metadata, we will need to use ``json.dumps()``
or write the dictionary to a JSON file:

.. code:: python

    >>> response = df_api.dataCreate('my important data',
                                     alias='my_cool_alias', # optional
                                     metadata=json.dumps(parameters), # also optional
                                     parent_id='root', # parent collection
                                    )

DataFed returns Google Protobuf messages in response to commands (both success and failure).
Let us take a look at an example response:

.. code:: python

    >>> print(response)

    (data {
       id: "d/30224875"
       title: "my important data"
       alias: "my_cool_alias"
       metadata: "{\"a\":4,\"b\":[1,2,-4,7.123],\"c\":\"Something important\",\"d\":{\"x\":14,\"y\":-19}}"
       repo_id: "repo/cades-cnms"
       size: 0.0
       ext_auto: true
       ct: 1605133166
       ut: 1605133166
       owner: "u/somnaths"
       creator: "u/somnaths"
       parent_id: "c/u_somnaths_root"
     }, 'RecordDataReply')

Though the content in these message objects are clearly laid out,
getting at specific components of the messages requires a tiny bit of extra work.
For example, if we wanted to get the record ID to be used for later transactions,
here's how we could go about it:

.. code:: python

    >>> record_id = response[0].data[0].id
    >>> print(record_id)

    'd/30224875'

Let's put the raw data into this record.
For the sake of simplicity, I'll just use the metadata as the data itself:

.. code:: python

    >>> with open('parameters.json', mode='w') as file_handle:
            json.dump(parameters, file_handle)

Putting the data file into record:
Note that this file must be located such that it is visible to the (default) globus endpoint

.. code:: python

    >>> put_resp = df_api.dataPut(record_id,
                                  './parameters.json')
    >>> print(put_resp)

    (item {
       id: "d/30224875"
       title: "my important data"
       size: 0.0
       owner: "u/somnaths"
     }
     task {
       id: "task/30225166"
       type: TT_DATA_PUT
       status: TS_READY
       client: "u/somnaths"
       step: 0
       steps: 2
       msg: "Pending"
       ct: 1605133526
       ut: 1605133526
       source: "1646e89e-f4f0-11e9-9944-0a8c187e8c12/Users/syz/Desktop/parameters.json"
       dest: "d/30224875"
     }, 'DataPutReply')

Viewing the record:
Clearly, you will notice the source and file extension have been updated:

.. code:: python

    >>> dv_resp = df_api.dataView(record_id)
    >>> prit(dv_resp)

    (data {
       id: "d/30224875"
       title: "my important data"
       alias: "my_cool_alias"
       metadata: "{\"a\":4,\"b\":[1,2,-4,7.123],\"c\":\"Something important\",\"d\":{\"x\":14,\"y\":-19}}"
       repo_id: "repo/cades-cnms"
       size: 86.0
       source: "1646e89e-f4f0-11e9-9944-0a8c187e8c12/Users/syz/Desktop/parameters.json"
       ext: ".json"
       ext_auto: true
       ct: 1605133166
       ut: 1605133539
       dt: 1605133539
       owner: "u/somnaths"
       creator: "u/somnaths"
       notes: 0
     }, 'RecordDataReply')

By default, the metadata in the response is a JSON string:

.. code:: python

    >>> dv_resp[0].data[0].metadata

    '{"a":4,"b":[1,2,-4,7.123],"c":"Something important","d":{"x":14,"y":-19}}'

In order to get back a python dictionary, use ``json.loads()``

.. code:: python

    >>> json.loads(dv_resp[0].data[0].metadata)

    {'a': 4,
     'b': [1, 2, -4, 7.123],
     'c': 'Something important',
     'd': {'x': 14, 'y': -19}}
